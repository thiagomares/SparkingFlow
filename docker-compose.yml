version: "3.8"

# Itens comuns para o Spark
x-spark-common: &spark-common
  image: bitnami/spark:latest
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
  networks:
    - airflow

# Itens comuns para o Airflow
x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  env_file:
    - .env
  volumes:
    - ./jobs:/opt/airflow/jobs
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
  depends_on:
    - postgres
    - mysql
  networks:
    - airflow

services:
  spark-master:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "${SPARK_MASTER_UI_PORT}:8080"
      - "${SPARK_MASTER_RPC_PORT}:7077"

  spark-worker:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:${SPARK_MASTER_RPC_PORT}
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_MASTER_URL: spark://spark-master:${SPARK_MASTER_RPC_PORT}
      PYSPARK_PYTHON: python3
      PYSPARK_DRIVER_PYTHON: python3
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop

  postgres:
    image: postgres:14.0
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    networks:
      - airflow

  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "${MYSQL_PORT}:3306"
    networks:
      - airflow

  webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    depends_on:
      - scheduler
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@mysql:${MYSQL_PORT}/${MYSQL_DATABASE}
      PYSPARK_PYTHON: python3
      PYSPARK_DRIVER_PYTHON: python3

  scheduler:
    <<: *airflow-common
    command: bash -c "airflow db migrate && airflow users create --username ${AIRFLOW_ADMIN_USERNAME} --firstname ${AIRFLOW_ADMIN_FIRSTNAME} --lastname ${AIRFLOW_ADMIN_LASTNAME} --role Admin --email ${AIRFLOW_ADMIN_EMAIL} --password ${AIRFLOW_ADMIN_PASSWORD} && airflow scheduler"

networks:
  airflow:
